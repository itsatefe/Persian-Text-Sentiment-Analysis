{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()\n",
    "i = 0\n",
    "for i in range(2,66):\n",
    "    if i != 3:\n",
    "        data = pd.read_excel('myExcel\\\\split_'+str(i)+'.xlsx')\n",
    "        data = data[['product_id','user_id','comment','label','file']]\n",
    "        dataset = dataset.append(data)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set of each excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_dataframe(identifier):\n",
    "    excel_list = []\n",
    "    i = 0 \n",
    "    for item in dataset.values:\n",
    "        \n",
    "        if item[4] == identifier:\n",
    "            excel_list.append(item) \n",
    "        \n",
    "        i += 1\n",
    "    excel_df =  pd.DataFrame(excel_list, columns=['product_id','user_id','comment','label','file'])\n",
    "    return excel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excel_df = excel_dataframe(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCHING IN COLUMN FOR PRODUCT ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prod_label(dataset,identifier):\n",
    "    product_list = []\n",
    "    #print(\"_____PRODUCT______\")\n",
    "    for item in dataset.values:\n",
    "        if  identifier == item[0] :\n",
    "            product_list.append(('p',int(item[3]),int(item[1])))\n",
    "    return product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_prod_label(dataset,56900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEARCHING IN COLUMN FOR USER ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_label(dataset,identifier):\n",
    "    user_list = []\n",
    "    #print(\"_____USER______\")\n",
    "    for item in dataset.values:\n",
    "        if  identifier == item[1] :\n",
    "            user_list.append(('u',int(item[3]),int(item[0])))\n",
    "    return user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_user_label(dataset,1423896)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user and prod status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status(which,dataset,identifier):\n",
    "    \n",
    "    result = []\n",
    "    temp_list = []\n",
    "    mid = 0\n",
    "    \n",
    "    which.lower()\n",
    "    if which == 'user':\n",
    "        my_list = get_user_label(dataset,identifier)\n",
    "        i = 0\n",
    "        for n in my_list:\n",
    "\n",
    "\n",
    "            for item in get_prod_label(dataset,my_list[i][2]):\n",
    "                mid += item[1]\n",
    "            result.append(mid / len(get_prod_label(dataset,my_list[i][2])) * 100)\n",
    "            mid = 0\n",
    "            i += 1\n",
    "\n",
    "        j=0\n",
    "        for n  in my_list:\n",
    "            if result[j] < -50:\n",
    "                if my_list[j][1] == -1:\n",
    "                    temp_list.append(0)\n",
    "                elif my_list[j][1] == 0:\n",
    "                    temp_list.append(1)\n",
    "                else:\n",
    "                    temp_list.append(2)\n",
    "\n",
    "            if  result[j] > -50 and result[j] < 50:\n",
    "                if my_list[j][1] == -1:\n",
    "                    temp_list.append(-1)\n",
    "                elif my_list[j][1] == 0:\n",
    "                    temp_list.append(0)\n",
    "                else:\n",
    "                    temp_list.append(1)\n",
    "            if result[j] > 50:\n",
    "                if my_list[j][1] == -1:\n",
    "                    temp_list.append(-2)\n",
    "                elif my_list[j][1] == 0:\n",
    "                    temp_list.append(-1)\n",
    "                else:\n",
    "                    temp_list.append(0)\n",
    "                    \n",
    "            j+=1\n",
    "            \n",
    "    elif which == 'product':\n",
    "        my_list = get_prod_label(dataset,identifier)\n",
    "        mid = 0\n",
    "        i = 0\n",
    "        for n in my_list:\n",
    "            for item in get_user_label(dataset,my_list[i][2]):\n",
    "                mid += item[1]\n",
    "\n",
    "            result.append(mid / len(get_user_label(dataset,my_list[i][2])) * 100)\n",
    "            mid = 0\n",
    "            i += 1\n",
    "            \n",
    "        j=0\n",
    "        for n  in my_list:\n",
    "            if result[j] <= -50:\n",
    "                if my_list[j][1] == -1:\n",
    "                    temp_list.append(0)\n",
    "                elif my_list[j][1] == 0:\n",
    "                    temp_list.append(1)\n",
    "                else:\n",
    "                    temp_list.append(2)\n",
    "\n",
    "            if result[j] > -50 and result[j] < 50:\n",
    "                if my_list[j][1] == -1:\n",
    "                    temp_list.append(-1)\n",
    "                elif my_list[j][1] == 0:\n",
    "                    temp_list.append(0)\n",
    "                else:\n",
    "                    temp_list.append(1)\n",
    "            if result[j] > 50:\n",
    "                if my_list[j][1] == -1:\n",
    "                    temp_list.append(-2)\n",
    "                elif my_list[j][1] == 0:\n",
    "                    temp_list.append(-1)\n",
    "                else:\n",
    "                    temp_list.append(0)\n",
    "                    \n",
    "            j+=1\n",
    "            \n",
    "    else:\n",
    "        sys.exit(\"just Enter 'user' or 'product'\")\n",
    "        \n",
    "    if len(my_list) == 0:\n",
    "        sys.exit(\"no comment for this product or user\")\n",
    "        \n",
    "\n",
    "    #Final result\n",
    "    res = sum(temp_list)  / len(temp_list)\n",
    "    if res <= -0.5:\n",
    "        if 'u' in my_list[0]:\n",
    "            print('Pessimistic person')\n",
    "        else:\n",
    "            print('Bad product')\n",
    "    elif res > -0.5 and res < 0.5:\n",
    "        if 'u' in my_list[0]:\n",
    "            print('Realistic person')\n",
    "        else:\n",
    "            print('Average product')\n",
    "    else:\n",
    "        if 'u' in my_list[0]:\n",
    "            print('Optimistic person')\n",
    "        else:\n",
    "            print('Good product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status('user',dataset,1423896)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential , load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from pickle import dump,load\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense , Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.wrappers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMENT TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=1000,split=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_tokenizer(dataset):\n",
    "    import pandas as pd\n",
    "\n",
    "    \n",
    "    #dataset.drop_duplicates(['comment'])\n",
    "    \n",
    "    stopword=stopwords_list()\n",
    "    index=[]\n",
    "    a=['1','2','3','4','5','6','7','8','9','Q','W','E','R','T','Y','U','I','O','P','A','S','D','F','N','G','H','J','K','L','Z','X','C','V','B','N','M','.',':','?','!','(',')',',',']','.',';','*','q','w','e','r','t','y','u','i','o','p','a','s','d','f','g','h','j','k','l','z','x','c','v','b','n','m','ØŸ']\n",
    "    h=len(a)\n",
    "    normalizer = Normalizer()\n",
    "    \n",
    "    i=0\n",
    "    X = []\n",
    "    for item in dataset['comment']:\n",
    "        if not isinstance(item, int) and not isinstance(item, float):\n",
    "            for it in range(0,h) :\n",
    "                 item=item.replace(a[it], \"\")\n",
    "            text = item.replace('\\u200c', ' ').replace('\\n', '').replace('\\r', '').replace('ÙŠ', 'ÛŒ').replace('Ùƒ', 'Ú©')        \n",
    "            e = normalizer.normalize(text)\n",
    "            tokens = word_tokenize(e)\n",
    "            l3 = [x for x in tokens if x not in stopword]\n",
    "            X.append(l3)\n",
    "        else:\n",
    "            index.append(i)\n",
    "        i += 1\n",
    "            \n",
    "    tokenizer.fit_on_texts(X)\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    return X ,index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_reader(dataset,index):\n",
    "    y = []\n",
    "    j=0\n",
    "    for item in dataset['label']:\n",
    "        if j not in index:\n",
    "            y.append(item)\n",
    "        j += 1\n",
    "        \n",
    "    import math\n",
    "    for n , label in enumerate(y):\n",
    "        if math.isnan(label):\n",
    "            y[n] = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,index = comment_tokenizer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = label_reader(dataset,index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ready_train(X,y):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.20,random_state=42)\n",
    "    \n",
    "    x_train = sequence.pad_sequences(x_train,maxlen=100)\n",
    "    x_test = sequence.pad_sequences(x_test,maxlen=100)\n",
    "  \n",
    "    x_test = np.array(x_test)\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    \n",
    "    x_test = np.array(x_test)\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    y_train = utils.to_categorical(y_train,3) \n",
    "    y_test = utils.to_categorical(y_test,3)\n",
    "    \n",
    "    \n",
    "    return x_train , x_test , y_train , y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test , y_train, y_test = get_ready_train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(10000,64))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.2)))\n",
    "    model.add(Bidirectional(LSTM(25, dropout=0.2)))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          640000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         66048     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50)                30800     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 737,001\n",
      "Trainable params: 737,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    " model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ParsIran\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40210 samples, validate on 10053 samples\n",
      "Epoch 1/3\n",
      "40210/40210 [==============================] - 133s 3ms/step - loss: 0.6228 - accuracy: 0.7581 - val_loss: 0.5546 - val_accuracy: 0.7868\n",
      "Epoch 2/3\n",
      "40210/40210 [==============================] - 136s 3ms/step - loss: 0.5457 - accuracy: 0.7898 - val_loss: 0.5499 - val_accuracy: 0.7894\n",
      "Epoch 3/3\n",
      "40210/40210 [==============================] - 132s 3ms/step - loss: 0.5262 - accuracy: 0.7977 - val_loss: 0.5476 - val_accuracy: 0.7908\n"
     ]
    }
   ],
   "source": [
    "training = model.fit(x_train,y_train,epochs=3,batch_size=50, validation_split=0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finalize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50263/50263 [==============================] - 277s 6ms/step - loss: 0.5342 - accuracy: 0.7943\n",
      "Epoch 2/3\n",
      "50263/50263 [==============================] - 277s 6ms/step - loss: 0.5154 - accuracy: 0.8019\n",
      "Epoch 3/3\n",
      "50263/50263 [==============================] - 273s 5ms/step - loss: 0.5012 - accuracy: 0.8075\n"
     ]
    }
   ],
   "source": [
    "final = model.fit(x_train,y_train,epochs=3,batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  91,  711,  319],\n",
       "       [  59, 7476,  513],\n",
       "       [  49,  820, 2528]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.argmax(axis=1),predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12566/12566 [==============================] - 7s 575us/step\n"
     ]
    }
   ],
   "source": [
    "test_score , test_acc = model.evaluate(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803358256816864"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentimentAnalysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer,open('my_simpletokenizer','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = load_model('sentimentAnalysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load(open('my_simpletokenizer','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test dataset (UNSEEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_standard(dataset):\n",
    "    x_test,index = comment_tokenizer(dataset)\n",
    "    y_test = label_reader(dataset,index)\n",
    "    x_test = sequence.pad_sequences(x_test,maxlen=50)\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = utils.to_categorical(y_test,3)\n",
    "    return x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(path):\n",
    "    data = pd.read_excel(path)\n",
    "    data = data[['comment','label']]\n",
    "    xt = []\n",
    "    yt = []\n",
    "    xt , yt = making_standard(data)\n",
    "    answer=sent_model.predict_classes(xt)\n",
    "    return confusion_matrix(yt.argmax(axis=1),answer) , xt , yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf_matrix , xt , yt = test_dataset('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_score , test_acc = sent_model.evaluate(xt, yt, verbose=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test single comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single():\n",
    "    X=[]\n",
    "    text = input('comment: ')\n",
    "    tokens = word_tokenize(text)\n",
    "    X.append(tokens)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = sequence.pad_sequences(X, maxlen=150)\n",
    "    result = sent_model.predict(X)\n",
    "    print(\"neutral  positive   negative\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    test_single()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
